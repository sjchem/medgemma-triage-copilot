# ──────────────────────────────────────────────────────────────
# MedGuard Triage Copilot – Environment Variables
# ──────────────────────────────────────────────────────────────
# Copy this to .env and fill in your values:
#   cp .env.example .env

# ── Stage 1: MedASR ────────────────────────────────────────────
# Fully local — no token needed. Weights are in models/asr/medasr_local/

# ── Stage 2: Clinical Structurer ───────────────────────────────
# Default backend is Ollama (local, no token needed).
# To switch to HF cloud endpoint, set backend: hf_endpoint in
# configs/model_config.yaml and uncomment below.
# STRUCTURER_ENDPOINT_URL=https://your-gemma-endpoint.huggingface.cloud

# Ollama base URL (only if running on a non-default port/host)
# OLLAMA_BASE_URL=http://localhost:11434

# ── Stage 3: MedGemma Triage Reasoner ──────────────────────────
# Vertex AI backend — your deployed endpoint in europe-west4
# 1. Enable Vertex AI API in GCP console
# 2. Authenticate: gcloud auth application-default login
#    OR: GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json
GOOGLE_CLOUD_PROJECT=your-gcp-project-id
GOOGLE_CLOUD_LOCATION=europe-west4

# Optional: override endpoint ID from code (already set in model_config.yaml)
# VERTEX_ENDPOINT_ID=mg-endpoint-300c3b17-8504-4683-9d8d-5a806ea762db

# Legacy HF endpoint (no longer needed — comment out)
# HF_API_TOKEN=your_hf_token_here
# TRIAGE_ENDPOINT_URL=https://your-medgemma-endpoint.huggingface.cloud

# ── General ────────────────────────────────────────────────────
LOG_LEVEL=INFO
