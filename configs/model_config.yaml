# ──────────────────────────────────────────────────────────────
# MedGuard Triage Copilot – Model Configuration
# ──────────────────────────────────────────────────────────────

# Stage 1 – MedASR: Speech-to-Text (Google Conformer CTC, 105M params)
# Model: https://huggingface.co/google/medasr
# Small enough to run locally – download once, load offline.
asr:
  model_id: "google/medasr"
  local_dir: "models/asr/medasr_local"   # Downloaded model weights
  device: "auto"                    # "cpu", "cuda", or "auto"
  torch_dtype: "float32"            # MedASR works best with float32
  chunk_length_s: 20                # Recommended by MedASR docs
  stride_length_s: 2                # Overlap between chunks
  sample_rate: 16000                # MedASR requires 16kHz mono
  use_pipeline: true                # true = pipeline API, false = direct model

# Stage 2 – Gemma: Clinical Structurer
# backend options:
#   "hf_endpoint"  – HuggingFace Inference Endpoint (cloud, set STRUCTURER_ENDPOINT_URL)
#   "ollama"       – Local Ollama via LangChain  (fast, fully offline)
structurer:
  backend: "ollama"                  # Switch to "hf_endpoint" for cloud
  # ── Ollama settings (used when backend: ollama) ──
  ollama_model: "gemma2:2b"          # Any model pulled in Ollama
  ollama_base_url: "http://localhost:11434"
  # ── HF Endpoint settings (used when backend: hf_endpoint) ──
  endpoint_url: null                 # Loaded from env: STRUCTURER_ENDPOINT_URL
  parameters:
    max_new_tokens: 512
    temperature: 0.1
    do_sample: false
  timeout: 120

# Stage 3 – MedGemma: Triage Reasoner
# backend options:
#   "vertex_ai"    – Google Cloud Vertex AI (recommended)
#                    Set vertex_endpoint_id → uses your deployed endpoint
#                    Leave vertex_endpoint_id blank → uses Model Garden API
#   "hf_endpoint"  – HuggingFace Dedicated Endpoint (legacy, slow)
triage:
  backend: "vertex_ai"
  # ── Dedicated deployed endpoint (your current setup) ──
  vertex_model: "medgemma-1.5-4b-it"
  vertex_location: "europe-west4"
  vertex_endpoint_id: "mg-endpoint-300c3b17-8504-4683-9d8d-5a806ea762db"
  # vertex_project is loaded from env: GOOGLE_CLOUD_PROJECT
  # ── HF Endpoint settings (used when backend: hf_endpoint) ──
  endpoint_url: null                       # Loaded from env: TRIAGE_ENDPOINT_URL
  parameters:
    max_new_tokens: 1024
    temperature: 0.2
    do_sample: false
  timeout: 120

# General settings
general:
  log_level: "INFO"
  max_retries: 3
  retry_delay_s: 2
